# Config
# Set to true to show commands, but NOT run anything.
dryrun: false
transferDbPrefix: "transfer_"
# This directory is appended to the 'clusters:...:hcfsNamespace' value to store the transfer package for hive export/import.
exportBaseDirPrefix: "/apps/hive/warehouse/export_"
overwriteTable: "true"
# Metadata is Optional, when not specified defaults are used.  Defaults below.
metadata:
  concurrency: 4
  strategy: DIRECT
# Storage is Optional, when not specified defaults are used.  Defaults below.
storage:
  concurrency: 4
  strategy: SQL
clusters:
  LOWER:
    # Set for Hive 1/2 environments
    legacyHive: "true"
    # Is the 'Hadoop COMPATIBLE File System' used to prefix data locations for this cluster.
    # It is mainly used as the transfer location for metadata (export)
    # If the primary storage for this cluster is 'hdfs' than use 'hdfs://...'
    # If the primary storage for this action is cloud storage, use the
    #    cloud storage prefix. IE: s3a://my_bucket
    hcfsNamespace: "<hdfs://namespace>"
    hiveServer2:
      # Standalone jar file used to connect via JDBC to the LOWER environment Hive Server 2
      # NOTE: Hive 3 jars will NOT work against Hive 1.  The protocol isn't compatible.
      jarFile: "<environment-specific-jdbc-standalone-driver>"
      uri: "<lower-cluster-jdbc-url>"
      # Optional when using Kerberos Auth (Requires Kerb Ticket)
      connectionPropertites:
        user: "<user>"
        password: "<password>"
  UPPER:
    legacyHive: "false"
    # Is the 'Hadoop COMPATIBLE File System' used to prefix data locations for this cluster.
    # It is mainly used to as a baseline for where "DATA" will be transfered in the
    # STORAGE stage.  The data location in the source location will be move to this
    # base location + the extended path where it existed in the source system.
    # The intent is to keep the data in the same relative location for this new cluster
    # as the old cluster.
    # If the LOWER and UPPER clusters are share the same cloud storage, then use the same
    # hcfs base location as the LOWER cluster.
    hcfsNamespace: "<hdfs://namespace>"
    partitionDiscovery:
      # Addition HMS configuration needed for this "discover.partitions"="true"
      #     Set by default on IMPORT EXTERNAL.
      auto: true
      initMSCK: true
    hiveServer2:
      # Standalone jar file used to connect via JDBC to the LOWER environment Hive Server 2
      jarFile: "<environment-specific-jdbc-standalone-driver>"
      uri: "<upper-cluster-jdbc-url>"
      # Optional when using Kerberos Auth (Requires Kerb Ticket)
      connectionProperties:
        user: "<username>"
        password: "<password>"
